[THIS IS STILL WORK IN PROGRESS]

The programming community might just be one of the least scientific communities out there. It's understandable, writing a million lines of code in one paradigm then rewriting it in another paradigm just to see which is better in that specific case is a pretty hard sell. Therefore, every decision about how to program is based almost entirely on theory. Programming is still a relatively new field, so experience in it is still relatively low. Unfortunately, I'm not much better myself, having been a programmer for only 11 or so years. But still, I've spent countless hours contemplating the fundamentals of programming paradigms, including their problems and the causes of said problems. Hopefully this is enough, at least for now, for my thoughts to be respectable.



Part 1

Anyone who has spent even a second in this community will know the paradigm that completely dominates everything: OOP. It needs no introduction, but it does not a solid definition. For the purposes of this work, my definition of an object-oriented program is: a program made of objects which achieve goals mainly by calling methods in other objects.

I'd love to give one big, fundamental reason for why OOP sucks, and though I can (it prioritizes the way humans think over the way programming should work), that doesn't directly explain why it's so bad. OOP is death by a thousand cuts, with countless tiny problems slowly degrading your productivity as scale increases. However, there's still one major problem that trumps all others, and it all has to do with control flow. It's the most important aspect of a program, which becomes obvious if you think for even just a second. When a user clicks a button, they don't care about the factory factories or any other patterns that you use, they care about the sequence of actions that occurs.

And here comes the problem: OOP is built off the idea that control flow doesn't matter. When you add a call to a method, in theory, you don't have to worry about that method's control flow because all the entangled managers and whatnot should handle everything. In reality, though, adding a call to an existing method is a huge gamble, because there's no telling what flags, events, and other systems will be affected.

This might seem harsh, but just think about it. The whole point of OOP is that, instead of a call graph of functions, you can think of your program on an object-to-object basis. Even back when OOP was still Alan Kay's original vision, it still undeniably assumed that control flow doesn't matter. And thought processes like this do have visible effects, such as inheritance in languages like Java ensuring that almost every variable has possibly modified control flow. But the main point is that OOP promises that you can think about an object's control flow completely seperately from other objects' control flows.

Although, there's something kinda strange that I need to address, which is that OOP (excluding inheritance) and procedural are mechanically identical. Just think about the semantics of function calls. In one, you effectively have `funtionName(arg1, arg2, ...)`, and in the other you have `arg1.functionName(arg2, ...)`. C++ literally transforms the second example into the first, because every single thing that you can do in OOP (except inheritence) can be recreated one to one with procedural code. And I don't mean you can accomplish the same tasks in both, all you have to do to change OOP into procedural is swap every method with a function and every object with a struct. So this brings up an interest question, why do many people (including me) say that OOP is better / worse than procedural when they're actually the same thing? As is often the case, the question is technically flawed. Yes they're mechanically identical, but they're extremely different mentally.

Let's take a quick detour. You have a sort of "working memory", which is an extremely short-term memory where you store what you're thinking about at any point in time. This working memory is extremely small, and obviously can't fit the entire inner workings of a program. Therefore, you have to take shortcuts that allow you to consider other parts of the program that deal with what you're actually working on. Programming paradigms can be thought of as a way to represent the important details of the program in a way that easily fits in your working memory. And finally, this is where prodecural and OOP differ. Even though they're mechanically the same, the way they represent the code completely transforms the way you think of a program. Again, OOP promises that you can think of your program on an object-to-object basis. You might want to say that modern programmers have realised that programs can't actually be made like this, but just look at, for example, any tutorial that uses OOP. I can guaruntee that it goes over the objects one at a time, only focusing on the interactions between them whenever it's absolutely required. And it's not even really a concious decision, the truth is that OOP's design literally goes out of its way to hide the program's control flow from you.

Now lets do a little recap. I'm saying that control flow is extremely important for software development (and that's an understatement), please take a minute to check if you agree. I'm also saying that OOP fundamentally disregards control flow, and again, please take a minute to check if you agree. And if you do agree to both statements, you have to agree that OOP is fundamentally flawed. But this is still just theory, does it have any effect in the real world?

Look back at the last time you programmed with OOP, I can pretty much guarantee it went like this: you set everything up with an ideal system of whatever design patterns you want, and with some luck, it might even work without any structural changes. But then comes the modifications and new features. Maybe you try to restructure everything to accommodate, or maybe you try to work the additions into the existing framework. But either way, there's always two stages here: the idealism stage where you build the code, trying to follow OOP's rules, and the reality stage where you use a debugger to fix the control flow and actually make it work.

When adding a new feature, you have to add any new associated data, then add the control flow for the feature. This cannot be changed. In theory you work on an object-to-object basis, but in reality you have to constantly jump between objects. Again, you CANNOT change the fact that you have to add features one control flow at a time. When adding code to an object, you have to keep in mind the effects of the methods you're calling, and you have to keep in mind how that object is used by other objects. Hopefully you fully agree that every time without fail, modifications to an object require modifications to connected objects, which would mean you agree that features have to be added one control flow at a time.

So, what if you agree that OOP does problematically disregard control flow, but you doubt that there are other substantial issues? Well then, let's talk about code reuse. You're taught that you should always reuse code, and you do, because that sounds like obviously good advice. In practice, though, reusing code can actually cause more trouble than it's worth. Just think about this: what if an existing function mostly does what you want, but not exactly? Creating a copy of the function is "bad programming practice", so you have to modify the function to fit your needs. Maybe add a boolean or enum to the arguments? Add a flag that's set in the object? Wait I know, you should use inheritance! None of these are ideal, and in the worst cases, you could end up with a function that only works on tuesdays. What's going on here is that you're often modifying code that doesn't concern the feature you're working on, even though that ideally should never happen. If that's done too much, sections of code can inderectly affect other sections of code that shouldn't be related at all. I've seen some applications, like the game BeamNG, have code that is so interconnected that saving a car's configuration changes the camera angle and exits fullscreen (yes, that was a real bug). But as always, what's other solutions are there? Consider the path where you do create a copy of the function, you can do whatever you want without worrying about unrelated code. And when you're done, you can even extract whatever was unedited so that you aren't even left with any duplicate code.

Now let me guess what you're thinking, "this isn't related to OOP, code reuse is a general principal". While true, just think about this: would it make sense to have two methods in an object that do basically the same thing? There's many times where the answer is yes, but it's not every time. For many methods, it feels very wrong to have more than one of it, and that means OOP subconsiously guides you towards code reuse. Whenever you need to call a method in an object, how often do you consider whether to clone the method for your personal use? Probably never, because that sounds ridiculous and completely goes against OOP's principles. However, it's often what needs to be done.

I haven't even gone over the "four pillars of OOP", because hopefully now it's clear how and why they fail. Encapsulation fails because control flow doesn't care about the bounderies of objects. Abstraction is the same as encapsulation, change my mind. I'm not going over inheritance. And finally, Polymorphism isn't restricted to OOP. There's also the problem of poor optimization, since OOP tends to spread data all over the heap, but you don't care do you?

But even if you completely agree that OOP is terrible, you should now have this very important question: if it's really that bad then why is it so widely used? Even though more and more people are starting to realize that OOP isn't as infallible as once thought, it still seems like programmers are attracted to OOP like it has its own gravitational pull. Once someone learns what it is, it's like other options simply no longer exist. I said earlier that OOP focuses on how humans naturally think, but once again, that doesn't directly explain this. I'd say that OOP is so widely used because it's comfy. Even knowing that OOP is bad, it's comfortable to know that your program is just a bunch of objects that talk to each other. OOP promises that you can work on one object at a time, and we believe it. No matter how often you need to coordinate control flow bewteen multiple objects, you always subconciously think that you'll only have to deal with one at a time. And besides, there's so many design patterns that surely you'll be able to deal with whatever problems pop up, right? Or maybe you can switch to whatever this year's "actual way to use OOP" is, I'm sure they got it right this time!

You likely always think that your implementation of OOP is the problem, and that your next project might just get it right. Everyone else uses it just fine, right? Everyone seems to agree that even if it has problems, OOP is the only practical way to build large-scale software. And like I said in the previous paragraph, it's just comfortable to think that you're just working with self-contained objects.

Like I said near the beginning, OOP is death by a thoughsand cuts. It throws away the fundamentals of what programming actually is, and problems from this crop up in so many ways that it's hard to list them all. But that bring up an interesting question, what then are the fundamentals of what programming actually is? Programming is where you have data, and you update the data. That's all it is. Yes you can (and should) add more to help curb the effects of scale, but it's extremely helpful to keep this in mind for both programming and theorising about programming. And who could've guessed, this shows another reason why OOP is fundamentally a bad idea. It hinders your ability to have data, with it being haphazardly spread among objects where everything is private, and it hinders your ability to update data, since you're constrained to the update methods that were created way before you actually knew what those updates would need to be.



Part 2

If programming is just defining data and how to update it, then ideally, your programming paradigm should focus on defining and updating data in the most practical way possible. Funilly enough, the first ever paradigms got this spot on. Procedural may be a little chaotic, but you can at least see what the program is actually doing. Functional programming had good intentions, but its obsession with correctness makes it impractical for most uses.

Data oriented program (DOP) focuses on completely seperating data and statements. Most data does not belong to specific functions, and even when it does, it's still dangerous to hide data within a specific function, namespace, etc.

One way to program is what I like to call "blob programming", where every piece of data is added to "the blob", and the main loop holds a list of updater functions which update the blob. It may sound scary, but it scales extremely well. There are updater functions for taking user input, updater functions for program logic, updater functions for rendering, etc. ECS is a subset of this, just being a bit more opinionated, and prioritizing more of smaller updaters instead of blob programming's fewer, larger updaters. I wouldn't count procedural programming as a subset of blob programmign, though, since pieces of data could just be static variables that have limited scope. The biggest problem I can see with blob programming is likely that you could easily accidentally mutate state that is essential for other lines of control flow, but this could be heavily diminished with good documentation.



Part 3

Finally, I want to quickly shed light on some other problems within the programming community. Let's talk about the Open-Closed principle, where code should be "open to entension but closed to modification". This idea is absolutely ridiculous, most obviously because there's a nearly infinite number of extensions that could be needed. Should you accomodate for each one? How much technical debt is that going to add? I've seen many people saying that premature optimization is the root of all evil, but no one ever talks about prematurely fixing problems, before they exist. If you always try to fix problems that don't exist yet, you'll innevitably fix problems that will never exist, adding technical debt for no reason. As anyone who has ever programmed knows, you always end up needing to modifying code anyway, and it's not even possible to follow this principle. Like I said before, features always have to be added one control flow at a time. Even people that advocate for the Open-Closed principle undoubtedly do still modify code, because they don't actually test to see if what they're saying is practical. If you didn't believe me when I said that the programming community is unscientific, hopefully you'll believe me now.

However, there's an ever bigger problem with the programming community, if you can believe it. We get way too excited about new and shiny featues, ideas, algorithms, tools, etc. Just look at the earliest Rust game engines, they were so eager to use Rust's new and shiny features that they forgot to actually be good. How many times have you yourself shoved in a design pattern you just learned of just because you thought it was really cool and you wanted an excuse to use it? How many people implemented an LSP for no other reason than they're a cool new way to create plugins? How many programming languages have been made because there was this one really cool feature that they wanted to have? I sure know that I'm often guilty of doing something because it's cool, and I've been guilty since I learned what a function is. This isn't actually very harmful as long as you can recognize that it's happening and stop it when needed, but it's extremely dangerous when it's happening subconciously. How much technical debt exists just because someone thought that their way of implenting something was really cool? How many layers of abstraction only exist because the person creating it thought it was cool? This might not have sounded like a big problem at first, but its consequences are close to dooming humanity. Does a car's accelerator really need 256,600 lines of code? Apparently someone at Toyota thought so (https://users.ece.cmu.edu/~koopman/pubs/koopman14_toyota_ua_slides.pdf p18).

As you've seen youself, I clearly hate OOP. Despite that, though, I have a confession to make... I use OOP in small quantities. Yes, this may be shocking, but I've found that there are times when using OOP is just fine. Go back to what I was saying about working memory, and how programming paradigms are just ways to represent the important details of a program in a way that easily fits in working memory. Now consider this: if a subsection of code is small enough that its entirety can fit within your working memory, does it really matter what paradigm you're using? This might also be shocking, but I'd be willing to say that OOP does have some benefits. Mostly, it makes parts of code less daunting to work with. If you have a small section of code where OOP gives benefits without any weaknesses, why not use it?

That might bring up this thought though: is it even possible to mix paradigms like that? Yes, it's absolutely possible. I consider function arguments to be an aspect of functional programming, so I'd say that you're already mixing OOP with FP. That part might be controversial, but hopfully you'd agree that small sections of OOP could be inserted into a procedural codebase. Just make sure to keep it small, because once you can't keep track of the control flow between objects, all of OOP's problems start showing up again.

But going to back to sillier topics, it's ridiculous that some programmers say that declaritive programming exists. Come on, just look at any "declaritive code". It's imperitive code just written pretentiously.
